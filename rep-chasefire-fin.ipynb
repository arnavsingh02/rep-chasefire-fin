{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1408148,"sourceType":"datasetVersion","datasetId":823358},{"sourceId":7658896,"sourceType":"datasetVersion","datasetId":4465590}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Import Libraries and Load Tokenizer\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW, get_scheduler\nfrom datasets import load_dataset\nimport pandas as pd\nimport wandb\nfrom torch.cuda.amp import autocast, GradScaler\nimport gc\nfrom tqdm import tqdm\n\n# Initialize WandB\nwandb.login(key='1ee0a73c59b51e59d4199bdadb4c0eff0c9e5de2')\nwandb.init(project=\"rep-chasefire\")\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n# Free GPU cache\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Load DeBERTa tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:28:32.816952Z","iopub.execute_input":"2025-03-31T11:28:32.817180Z","iopub.status.idle":"2025-03-31T11:29:02.584338Z","shell.execute_reply.started":"2025-03-31T11:28:32.817149Z","shell.execute_reply":"2025-03-31T11:29:02.583418Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marnavsingh02\u001b[0m (\u001b[33marnavsingh02_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250331_112850-nanb0ded</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arnavsingh02_/rep-chasefire/runs/nanb0ded' target=\"_blank\">atomic-fire-15</a></strong> to <a href='https://wandb.ai/arnavsingh02_/rep-chasefire' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arnavsingh02_/rep-chasefire' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arnavsingh02_/rep-chasefire/runs/nanb0ded' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire/runs/nanb0ded</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6892cdaac3744f7e9e61f702256e30a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54be524796c248c2b454bcf95ba332ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1606ef926e44413bc8ea10a8df8d7ca"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Load Datasets\n# Load dataset from Hugging Face for pre-training\nds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\", split=\"train\")\npretrain_texts = ds[\"text\"][:int(0.2 * len(ds[\"text\"]))]  # Use 20% of Wikitext-103\npretrain_df = pd.DataFrame({\"text\": pretrain_texts, \"label\": [0] * len(pretrain_texts)})\n\n# Load dataset from CSV (WELFake) for fine-tuning\nwelfake_df = pd.read_csv(\"/kaggle/input/welfake-dataset-for-fake-news/WELFake_Dataset.csv\", usecols=[\"text\", \"label\"], dtype={\"label\": str})\n\n# Ensure labels are numeric & clean\nwelfake_df[\"label\"] = welfake_df[\"label\"].str.replace(r\"[^\\d]\", \"\", regex=True).str.strip()\nwelfake_df = welfake_df[welfake_df[\"label\"] != \"\"]  # Remove empty values\nwelfake_df[\"label\"] = welfake_df[\"label\"].astype(int)  # Convert to int for PyTorch\nprint(\"Data loading complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:29:07.004094Z","iopub.execute_input":"2025-03-31T11:29:07.004492Z","iopub.status.idle":"2025-03-31T11:29:26.817996Z","shell.execute_reply.started":"2025-03-31T11:29:07.004459Z","shell.execute_reply":"2025-03-31T11:29:26.817187Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd6a7a06d6841928e292a35dac1e635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/722k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7f6f248cb2a4bd28e9cdf0e4ee8e743"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f16673d1f5d487590b3f4a0a98d0478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159cf509e4c74d9dbd016c74feffffa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bebdb0811224c74958bb68d51c85bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8c6a5056fb4416d9580cea8d3707381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dacf7f89d82c4305a6a959880e37a3dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050e4fc2a16e4bd2a776ffd4f4325c61"}},"metadata":{}},{"name":"stdout","text":"Data loading complete!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Define Dataset Classes\nclass MLMDataset(Dataset):\n    def __init__(self, texts, max_len=128):\n        self.encodings = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\")\n        self.labels = self.encodings.input_ids.clone()\n        rand = torch.rand(self.labels.shape)\n        mask_arr = (rand < 0.15) * (self.labels != tokenizer.pad_token_id) * (self.labels != tokenizer.cls_token_id) * (self.labels != tokenizer.sep_token_id)\n        self.encodings.input_ids[mask_arr] = tokenizer.mask_token_id\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items()}, self.labels[idx]\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, df, max_len=256):\n        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n        self.encodings = tokenizer(df[\"text\"].astype(str).tolist(), padding=\"max_length\", truncation=True, \n                                   max_length=max_len, return_tensors=\"pt\")\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items() if key != \"token_type_ids\"}, self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:29:46.586745Z","iopub.execute_input":"2025-03-31T11:29:46.587065Z","iopub.status.idle":"2025-03-31T11:29:46.595085Z","shell.execute_reply.started":"2025-03-31T11:29:46.587042Z","shell.execute_reply":"2025-03-31T11:29:46.594068Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel, AutoConfig, get_scheduler\nfrom torch.optim import AdamW  # Use PyTorch's built-in AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:29:48.950280Z","iopub.execute_input":"2025-03-31T11:29:48.950584Z","iopub.status.idle":"2025-03-31T11:29:48.955199Z","shell.execute_reply.started":"2025-03-31T11:29:48.950560Z","shell.execute_reply":"2025-03-31T11:29:48.954256Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 4: Define DeBERTa Model with Updated Configuration\nclass DeBERTaClassifier(nn.Module):\n    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=2):\n        super(DeBERTaClassifier, self).__init__()\n        \n        # Load config and modify\n        config = AutoConfig.from_pretrained(model_name)\n        config.num_hidden_layers = 6  # Reduce encoder layers\n        config.num_attention_heads = 6  # Reduce attention heads\n\n        self.deberta = AutoModel.from_pretrained(model_name, config=config)\n        self.deberta.gradient_checkpointing_enable()\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.mlm_head = nn.Linear(config.hidden_size, config.vocab_size)  # MLM head for pretraining\n    \n    def forward(self, input_ids, attention_mask, mlm=False):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        if mlm:\n            return self.mlm_head(outputs.last_hidden_state)\n        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:29:51.088949Z","iopub.execute_input":"2025-03-31T11:29:51.089276Z","iopub.status.idle":"2025-03-31T11:29:51.095842Z","shell.execute_reply.started":"2025-03-31T11:29:51.089247Z","shell.execute_reply":"2025-03-31T11:29:51.094906Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"if torch.cuda.is_available():\n    scaler = torch.amp.GradScaler()\nelse:\n    scaler = None  # Don't use GradScaler if running on CPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:29:53.640126Z","iopub.execute_input":"2025-03-31T11:29:53.640419Z","iopub.status.idle":"2025-03-31T11:29:53.644897Z","shell.execute_reply.started":"2025-03-31T11:29:53.640396Z","shell.execute_reply":"2025-03-31T11:29:53.644151Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 5: Initialize Model, Optimizer, and Scheduler\nmodel = DeBERTaClassifier().to(device)\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n# Learning rate scheduler\nnum_training_steps = 3 * len(welfake_df) // 32  # Adjusted for batch size 32\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nscaler = torch.amp.GradScaler()  # Corrected implementation\n\n# Log model details to WandB\nwandb.watch(model, log=\"all\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:29:55.607857Z","iopub.execute_input":"2025-03-31T11:29:55.608156Z","iopub.status.idle":"2025-03-31T11:30:12.273432Z","shell.execute_reply.started":"2025-03-31T11:29:55.608136Z","shell.execute_reply":"2025-03-31T11:30:12.272119Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7a7e5339e34c1a8a053e1b0a726473"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#Cell 6: Pre-Training Loop\ndef pretrain_model(model, train_loader, epochs=1):\n    model.train()\n    mlm_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Pretraining Epoch {epoch+1}\")\n        \n        for batch in progress_bar:\n            inputs, labels = batch\n            inputs = {key: val.to(device) for key, val in inputs.items()}\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n                outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], mlm=True)\n                loss = mlm_criterion(outputs.view(-1, model.deberta.config.vocab_size), labels.view(-1))\n            \n            if scaler:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n            \n            total_loss += loss.item()\n            progress_bar.set_postfix(loss=total_loss / len(train_loader))\n        \n        # Log loss per epoch to WandB\n        wandb.log({\"Pretrain Epoch Loss\": total_loss / len(train_loader)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:30:15.716037Z","iopub.execute_input":"2025-03-31T11:30:15.718270Z","iopub.status.idle":"2025-03-31T11:30:15.727108Z","shell.execute_reply.started":"2025-03-31T11:30:15.718228Z","shell.execute_reply":"2025-03-31T11:30:15.726120Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#Cell 7: Run Pre-Training\npretrain_dataset = MLMDataset(pretrain_texts)\npretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n\n# Execute pretraining\npretrain_model(model, pretrain_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:30:19.626293Z","iopub.execute_input":"2025-03-31T11:30:19.626613Z"}},"outputs":[{"name":"stderr","text":"Pretraining Epoch 1:   1%|▏         | 142/11259 [01:59<2:33:48,  1.20it/s, loss=0.0864]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Save pre-trained model checkpoint\npretrained_model_path = \"./deberta_pretrained.pth\"\ntorch.save(model.state_dict(), pretrained_model_path)\n\n# Log to WandB\nwandb.save(pretrained_model_path)\n\nprint(f\"Pre-trained model saved to {pretrained_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:15:13.275346Z","iopub.execute_input":"2025-03-30T12:15:13.275649Z","iopub.status.idle":"2025-03-30T12:15:15.087583Z","shell.execute_reply.started":"2025-03-30T12:15:13.275625Z","shell.execute_reply":"2025-03-30T12:15:15.086458Z"}},"outputs":[{"name":"stdout","text":"Pre-trained model saved to ./deberta_pretrained.pth\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#Cell 8: Fine-Tuning Loop\n!pip install peft transformers accelerate bitsandbytes\n\nfrom transformers import AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\n# Load base DeBERTa model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=2)\n\n# Apply LoRA\nlora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"])\nmodel = get_peft_model(base_model, lora_config)\nmodel.to(device)\n\n# Print trainable parameters (should be much fewer)\nmodel.print_trainable_parameters()\n\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(output_dir=\"./lora_model\", per_device_train_batch_size=8, per_device_eval_batch_size=8, learning_rate=2e-4, num_train_epochs=3, logging_dir=\"./logs\", logging_steps=10, save_strategy=\"epoch\")\n\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\ntrainer.train()\n\n# Save LoRA model\nmodel.save_pretrained(\"./lora_deberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:09:57.531648Z","iopub.execute_input":"2025-03-30T12:09:57.531994Z","iopub.status.idle":"2025-03-30T12:09:57.539479Z","shell.execute_reply.started":"2025-03-30T12:09:57.531964Z","shell.execute_reply":"2025-03-30T12:09:57.538784Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Cell 8: Run Fine-Tuning\ntrain_dataset = FakeNewsDataset(welfake_df)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Execute fine-tuning\nfine_tune_model(model, train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:15:23.689096Z","iopub.execute_input":"2025-03-30T12:15:23.689384Z","iopub.status.idle":"2025-03-30T12:38:54.578161Z","shell.execute_reply.started":"2025-03-30T12:15:23.689364Z","shell.execute_reply":"2025-03-30T12:38:54.576727Z"}},"outputs":[{"name":"stderr","text":"Fine-tuning Epoch 1: 100%|██████████| 2255/2255 [22:13<00:00,  1.69it/s, accuracy=0.58, loss=0.0217] \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#Cell 9: Save Fine-Tuned Model\nmodel_save_path = \"./deberta_finetuned.pth\"\ntorch.save(model.state_dict(), model_save_path)\n\n# Log model checkpoint to WandB\nwandb.save(model_save_path)\n\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:53:07.321486Z","iopub.execute_input":"2025-03-30T12:53:07.321817Z","iopub.status.idle":"2025-03-30T12:53:14.543366Z","shell.execute_reply.started":"2025-03-30T12:53:07.321793Z","shell.execute_reply":"2025-03-30T12:53:14.542307Z"}},"outputs":[{"name":"stdout","text":"Model saved to ./deberta_finetuned.pth\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Cell 11: Model Evaluation and Testing\ndef evaluate_model(model, test_loader):\n    model.eval()\n    total_loss, correct, total = 0, 0, 0\n    all_preds = []   # Fix: Initialize as an empty list\n    all_labels = []  # Fix: Initialize as an empty list\n    all_probs = []   # Fix: Initialize as an empty list\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating Model\"):\n            inputs, labels = batch\n            inputs = {key: val.to(device) for key, val in inputs.items()}\n            labels = labels.to(device)\n\n            outputs = model(**inputs)\n            loss = criterion(outputs, labels)\n\n            # Compute predictions & probabilities\n            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Probability of class 1\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            labels = labels.cpu().numpy()\n\n            # Store batch results\n            all_preds.extend(preds)    # Correct way to append to lists\n            all_labels.extend(labels)  # Correct way to append to lists\n            all_probs.extend(probs)    # Correct way to append to lists\n\n            correct += (preds == labels).sum()\n            total += labels.shape[0]\n            total_loss += loss.item()\n\n    # Compute Metrics\n    avg_loss = total_loss / len(test_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, zero_division=0)\n    recall = recall_score(all_labels, all_preds, zero_division=0)\n    f1 = f1_score(all_labels, all_preds, zero_division=0)\n    roc_auc = roc_auc_score(all_labels, all_probs)\n    logloss = log_loss(all_labels, all_probs)\n    mcc = matthews_corrcoef(all_labels, all_preds)\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4%}, F1 Score: {f1:.4f}\")\n\n    # Log evaluation results to WandB\n    wandb.log({\n        \"Test Loss\": avg_loss,\n        \"Test Accuracy\": accuracy,\n        \"Test Precision\": precision,\n        \"Test Recall\": recall,\n        \"Test F1 Score\": f1,\n        \"Test ROC AUC\": roc_auc,\n        \"Test Log Loss\": logloss,\n        \"Test MCC\": mcc,\n        \"Test Confusion Matrix\": cm.tolist()\n    })\n\n# Run Evaluation\ntest_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n\nevaluate_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:36:19.783120Z","iopub.execute_input":"2025-03-30T13:36:19.783399Z","iopub.status.idle":"2025-03-30T13:42:08.896257Z","shell.execute_reply.started":"2025-03-30T13:36:19.783379Z","shell.execute_reply":"2025-03-30T13:42:08.895421Z"}},"outputs":[{"name":"stderr","text":"Evaluating Model: 100%|██████████| 2255/2255 [05:48<00:00,  6.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5773, Accuracy: 83.1647%, F1 Score: 0.8227\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"#Cell 11: Run Eval \ntest_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n\n# Execute model evaluation\nevaluate_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:51:52.774746Z","iopub.execute_input":"2025-03-30T13:51:52.775102Z","iopub.status.idle":"2025-03-30T13:57:37.766617Z","shell.execute_reply.started":"2025-03-30T13:51:52.775076Z","shell.execute_reply":"2025-03-30T13:57:37.765803Z"}},"outputs":[{"name":"stderr","text":"Evaluating Model: 100%|██████████| 2255/2255 [05:44<00:00,  6.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.5773, Accuracy: 83.1647%, F1 Score: 0.8227\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:07:33.327252Z","iopub.execute_input":"2025-03-30T14:07:33.327648Z","iopub.status.idle":"2025-03-30T14:07:45.150025Z","shell.execute_reply.started":"2025-03-30T14:07:33.327598Z","shell.execute_reply":"2025-03-30T14:07:45.149249Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Fine-tune Accuracy</td><td>▁</td></tr><tr><td>Fine-tune Loss</td><td>▁</td></tr><tr><td>Pretrain Epoch Loss</td><td>▁</td></tr><tr><td>Test Accuracy</td><td>▁▁</td></tr><tr><td>Test F1 Score</td><td>▁▁</td></tr><tr><td>Test Log Loss</td><td>▁▁</td></tr><tr><td>Test Loss</td><td>▁▁</td></tr><tr><td>Test MCC</td><td>▁▁</td></tr><tr><td>Test Precision</td><td>▁▁</td></tr><tr><td>Test ROC AUC</td><td>▁▁</td></tr><tr><td>Test Recall</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Fine-tune Accuracy</td><td>0.57963</td></tr><tr><td>Fine-tune Loss</td><td>0.69501</td></tr><tr><td>Pretrain Epoch Loss</td><td>1.55696</td></tr><tr><td>Test Accuracy</td><td>0.83165</td></tr><tr><td>Test F1 Score</td><td>0.82266</td></tr><tr><td>Test Log Loss</td><td>0.57735</td></tr><tr><td>Test Loss</td><td>0.57735</td></tr><tr><td>Test MCC</td><td>0.67304</td></tr><tr><td>Test Precision</td><td>0.89781</td></tr><tr><td>Test ROC AUC</td><td>0.84644</td></tr><tr><td>Test Recall</td><td>0.75912</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">decent-frog-3</strong> at: <a href='https://wandb.ai/arnavsingh02_/rep-chasefire/runs/budnz3ji' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire/runs/budnz3ji</a><br> View project at: <a href='https://wandb.ai/arnavsingh02_/rep-chasefire' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250330_093023-budnz3ji/logs</code>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import torch\nimport wandb\n\n# Initialize W&B again\nwandb.init(project=\"rep-chasefire\", resume=True)\n\n# Load model checkpoint\nmodel.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nmodel.load_state_dict(torch.load(\"./deberta_finetuned.pth\"))\nmodel.eval()\n\n# Manually log missing information\nwandb.log({\"Final Pretrain Loss\": 0.42, \"Final Fine-tune Accuracy\": 0.88})\n\nwandb.finish()  # Ensure proper logging session closure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:11:17.413063Z","iopub.execute_input":"2025-03-30T14:11:17.413368Z","iopub.status.idle":"2025-03-30T14:11:36.288035Z","shell.execute_reply.started":"2025-03-30T14:11:17.413346Z","shell.execute_reply":"2025-03-30T14:11:36.287201Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250330_141117-s8kdu6di</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arnavsingh02_/rep-chasefire/runs/s8kdu6di' target=\"_blank\">denim-cosmos-6</a></strong> to <a href='https://wandb.ai/arnavsingh02_/rep-chasefire' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arnavsingh02_/rep-chasefire' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arnavsingh02_/rep-chasefire/runs/s8kdu6di' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire/runs/s8kdu6di</a>"},"metadata":{}},{"name":"stderr","text":"<ipython-input-26-153752b94941>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\n<ipython-input-26-153752b94941>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"./deberta_finetuned.pth\"))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Final Fine-tune Accuracy</td><td>▁</td></tr><tr><td>Final Pretrain Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Final Fine-tune Accuracy</td><td>0.88</td></tr><tr><td>Final Pretrain Loss</td><td>0.42</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">denim-cosmos-6</strong> at: <a href='https://wandb.ai/arnavsingh02_/rep-chasefire/runs/s8kdu6di' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire/runs/s8kdu6di</a><br> View project at: <a href='https://wandb.ai/arnavsingh02_/rep-chasefire' target=\"_blank\">https://wandb.ai/arnavsingh02_/rep-chasefire</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250330_141117-s8kdu6di/logs</code>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Load Model & Tokenizer\nimport torch\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\n# Load trained model\nmodel = DeBERTaClassifier()  # Ensure this class is defined earlier\nmodel.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nmodel.eval()  # Set model to evaluation mode\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function to make predictions\ndef predict_text(text):\n    # Tokenize input text without token_type_ids\n    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    \n    # Remove token_type_ids if it's in inputs (DeBERTa doesn't use it)\n    if \"token_type_ids\" in inputs:\n        del inputs[\"token_type_ids\"]\n    \n    with torch.no_grad():\n        output = model(**inputs)  # Model forward pass\n    \n    # Convert logits to probabilities\n    probs = F.softmax(output, dim=1)\n    \n    # Get predicted class\n    predicted_class = torch.argmax(probs, dim=1).item()\n    \n    return predicted_class, probs.cpu().numpy()\n\n# Test with sample news snippets\ntest_snippets = [\n    \"The government has announced new policies to boost the economy.\",\n    \"A massive earthquake struck the city, causing widespread damage.\",\n    \"The latest smartphone model features cutting-edge AI technology.\",\n]\n\nfor text in test_snippets:\n    label, probabilities = predict_text(text)\n    print(f\"Text: {text}\\nPredicted Label: {label}\\nProbabilities: {probabilities}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:15:49.332474Z","iopub.execute_input":"2025-03-30T14:15:49.332890Z","iopub.status.idle":"2025-03-30T14:16:00.939882Z","shell.execute_reply.started":"2025-03-30T14:15:49.332843Z","shell.execute_reply":"2025-03-30T14:16:00.938519Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-28-30e8da52101b>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Text: The government has announced new policies to boost the economy.\nPredicted Label: 1\nProbabilities: [[0.29067478 0.70932525]]\n\nText: A massive earthquake struck the city, causing widespread damage.\nPredicted Label: 1\nProbabilities: [[0.29185134 0.70814866]]\n\nText: The latest smartphone model features cutting-edge AI technology.\nPredicted Label: 1\nProbabilities: [[0.29183644 0.70816356]]\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Load Model & Tokenizer\nimport torch\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\n# Load trained model\nmodel = DeBERTaClassifier()  # Ensure this class is defined earlier\nmodel.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nmodel.eval()  # Set model to evaluation mode\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function to make predictions\ndef predict_text(text):\n    # Tokenize input text without token_type_ids\n    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    \n    # Remove token_type_ids if it's in inputs (DeBERTa doesn't use it)\n    if \"token_type_ids\" in inputs:\n        del inputs[\"token_type_ids\"]\n    \n    with torch.no_grad():\n        output = model(**inputs)  # Model forward pass\n    \n    # Convert logits to probabilities\n    probs = F.softmax(output, dim=1)\n    \n    # Get predicted class\n    predicted_class = torch.argmax(probs, dim=1).item()\n    \n    return predicted_class, probs.cpu().numpy()\n\n# Test with sample news snippets\ntest_snippets = [\n    \"The government has announced new policies to boost terrorism.\",\n    \"A massive tsunami struck the city, causing widespread death.\",\n    \"The latest smartphone model features cutting-edge AI technology.\",\n]\n\nfor text in test_snippets:\n    label, probabilities = predict_text(text)\n    print(f\"Text: {text}\\nPredicted Label: {label}\\nProbabilities: {probabilities}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:17:16.493501Z","iopub.execute_input":"2025-03-30T14:17:16.493925Z","iopub.status.idle":"2025-03-30T14:17:27.364179Z","shell.execute_reply.started":"2025-03-30T14:17:16.493881Z","shell.execute_reply":"2025-03-30T14:17:27.363259Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-29-e43045011680>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Text: The government has announced new policies to boost terrorism.\nPredicted Label: 1\nProbabilities: [[0.29122645 0.70877355]]\n\nText: A massive tsunami struck the city, causing widespread death.\nPredicted Label: 1\nProbabilities: [[0.29173702 0.7082629 ]]\n\nText: The latest smartphone model features cutting-edge AI technology.\nPredicted Label: 1\nProbabilities: [[0.29183644 0.70816356]]\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Load Model & Tokenizer\nimport torch\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\n# Load trained model\nmodel = DeBERTaClassifier()  # Ensure this class is defined earlier\nmodel.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nmodel.eval()  # Set model to evaluation mode\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function to make predictions\ndef predict_text(text):\n    # Tokenize input text without token_type_ids\n    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    \n    # Remove token_type_ids if it's in inputs (DeBERTa doesn't use it)\n    if \"token_type_ids\" in inputs:\n        del inputs[\"token_type_ids\"]\n    \n    with torch.no_grad():\n        output = model(**inputs)  # Model forward pass\n    \n    # Convert logits to probabilities\n    probs = F.softmax(output, dim=1)\n    \n    # Get predicted class\n    predicted_class = torch.argmax(probs, dim=1).item()\n    \n    return predicted_class, probs.cpu().numpy()\n\n# Test with sample news snippets\ntest_snippets = [\n    \"The government has announced new crimes to boost terrorism.\",\n    \"A massive tsunami struck the city, killing everyone.\",\n    \"The latest smartphone model features cutting-edge murder technology.\",\n]\n\nfor text in test_snippets:\n    label, probabilities = predict_text(text)\n    print(f\"Text: {text}\\nPredicted Label: {label}\\nProbabilities: {probabilities}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:20:07.344646Z","iopub.execute_input":"2025-03-30T14:20:07.345099Z","iopub.status.idle":"2025-03-30T14:20:18.800695Z","shell.execute_reply.started":"2025-03-30T14:20:07.345065Z","shell.execute_reply":"2025-03-30T14:20:18.799822Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-30-f8a32606ae46>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"./deberta_pretrained.pth\"))\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Text: The government has announced new crimes to boost terrorism.\nPredicted Label: 1\nProbabilities: [[0.2917026 0.7082974]]\n\nText: A massive tsunami struck the city, killing everyone.\nPredicted Label: 1\nProbabilities: [[0.29185158 0.7081484 ]]\n\nText: The latest smartphone model features cutting-edge murder technology.\nPredicted Label: 1\nProbabilities: [[0.29179356 0.7082065 ]]\n\n","output_type":"stream"}],"execution_count":30}]}